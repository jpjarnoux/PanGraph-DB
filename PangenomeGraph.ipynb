{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb9b0e84",
   "metadata": {},
   "source": [
    "# Pangenomes graph database construction\n",
    "\n",
    "## Purpose\n",
    "This notebook allows to reproduce the construction of a Neo4J graph database with 10 ESKAPE pangenome. The code behind was developped in the ICDE conference paper submisson.\n",
    "\n",
    "## Notebook organisation\n",
    "This notebook will be split in X part\n",
    "\n",
    "1. Setup the notebook\n",
    "2. Import pangenomes to a neo4j database\n",
    "3. Graph Queries, experimental evaluation\n",
    "4. Supplementary code\n",
    "\n",
    "## Methodology\n",
    "Quickly describe assumptions and processing steps.\n",
    "\n",
    "## Results\n",
    "Thanks to this code it's possible to load multiple pangenomes and their similarities to make possible a new step in the field of comparative genomics.\n",
    "\n",
    "## Suggested next steps\n",
    "State suggested next steps, based on results obtained in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3428c268",
   "metadata": {},
   "source": [
    "# Setup\n",
    "## Environment configuration\n",
    "To begin, note that you must have an empty Neo4J DMBS (version 4.4.11) open and available with the APOC plugin install (version 4.4.0.10).\n",
    "\n",
    "To execute the following script, you will need to install some packages. They're listed in the following conda environment file. The *in development* version of PPanGGOLiN is required to satisfy some feature and pangenomes compatibility.\n",
    "\n",
    "To install the conda environment in jupyter kernel, please copy and paste the following code in your terminal:\n",
    "```\n",
    "conda update -n base -c defaults conda -y\n",
    "conda env create --file conda-env.yml\n",
    "conda init bash\n",
    "conda activate pangraph\n",
    "pip install --user ipykernel\n",
    "python -m ipykernel install --user --name=pangraph\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clonage dans 'PPanGGOLiN'...\r\n",
      "remote: Enumerating objects: 6007, done.\u001B[K\r\n",
      "remote: Counting objects: 100% (175/175), done.\u001B[K\r\n",
      "remote: Compressing objects: 100% (113/113), done.\u001B[K\r\n",
      "remote: Total 6007 (delta 81), reused 104 (delta 50), pack-reused 5832\u001B[K\r\n",
      "Réception d'objets: 100% (6007/6007), 130.20 Mio | 25.96 Mio/s, fait.\r\n",
      "Résolution des deltas: 100% (3446/3446), fait.\r\n",
      "Processing ./PPanGGOLiN\r\n",
      "  Preparing metadata (setup.py) ... \u001B[?25ldone\r\n",
      "\u001B[?25hBuilding wheels for collected packages: ppanggolin\r\n",
      "  Building wheel for ppanggolin (setup.py) ... \u001B[?25ldone\r\n",
      "\u001B[?25h  Created wheel for ppanggolin: filename=ppanggolin-1.2.105-cp38-cp38-linux_x86_64.whl size=3538076 sha256=ed32c48b935a0cff6750f76eaa2bbea49108f4952a97d6615cb613c55b1c5d17\r\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-73e3yfjr/wheels/c0/09/87/147f46fa9951bc20911b5efff11c297269a1f8cea4752a77e8\r\n",
      "Successfully built ppanggolin\r\n",
      "Installing collected packages: ppanggolin\r\n",
      "  Attempting uninstall: ppanggolin\r\n",
      "    Found existing installation: ppanggolin 1.2.105\r\n",
      "    Uninstalling ppanggolin-1.2.105:\r\n",
      "      Successfully uninstalled ppanggolin-1.2.105\r\n",
      "Successfully installed ppanggolin-1.2.105\r\n"
     ]
    }
   ],
   "source": [
    "!git clone -b release1.3 https://github.com/labgem/PPanGGOLiN.git\n",
    "!pip install PPanGGOLiN/."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Library import\n",
    "We import all the common required Python libraries to execute all the script"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47eab96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# default libraries\n",
    "import logging\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "# installed libraries\n",
    "from py2neo import Graph\n",
    "\n",
    "# local libraries\n",
    "from script.python.utils import check_tsv_sanity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce7c84f",
   "metadata": {},
   "source": [
    "# Parameter definition\n",
    "We set all relevant parameters for our notebook. By convention, parameters are uppercase, while all the \n",
    "other variables follow Python's guidelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "GBFF=\"data/GBFF\"  # Genome used to construct pangenomes\n",
    "os.environ[\"GBFF\"] = GBFF\n",
    "PANGENOMES=\"original_data/pangenomes\"  # Directory with our pangenomes\n",
    "os.environ[\"PANGENOMES\"] = PANGENOMES\n",
    "GF=\"data/GF_fasta\"  # Directory with for all pangenomes the DNA sequences of all gene families\n",
    "SIMILARITY=\"original_data/similarity\"\n",
    "\n",
    "PANGENOMES_TSV=Path(f\"{PANGENOMES}/organism_eskape_2.list\")\n",
    "SIMILARITIES=Path(f\"{SIMILARITY}/clustering2/clust_concat.tsv\")\n",
    "\n",
    "#Neo4J paramaeters\n",
    "URI=\"bolt://localhost:7687\"\n",
    "USER=\"neo4j\"\n",
    "PWD=\"PANORAMA2022\"\n",
    "\n",
    "#Exec parameters\n",
    "CPU=6\n",
    "os.environ[\"CPU\"]=str(CPU)\n",
    "BATCH_SIZE=1000\n",
    "\n",
    "pangenomes = check_tsv_sanity(PANGENOMES_TSV)  # Create a dictionnary with path file and information to pangenomes.\n",
    "graph = Graph(uri=URI, user=USER, password=PWD)\n",
    "# graph.delete_all()  # make sure the graph is empty"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Import pangenomes to a neo4j database\n",
    "## Translation pangenome in dictionnary to export in Graph database\n",
    "\n",
    "The first step is to translate our pangenomes in a data structure adapted to export into our Graph database."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "from script.python import Pangenome\n",
    "from script.python.translate import write_families, write_organisms, write_spot, write_rgp, write_modules\n",
    "\n",
    "def create_dict(pangenome: Pangenome) -> dict:\n",
    "    \"\"\"Create a dictionary with the pangenome content to export one pangenome into a graph database\n",
    "    :param pangenome: A pangenome construct thanks to PPanGGOLiN\n",
    "\n",
    "    :return: Compatible dictionary to export in graph database, corresponding to a pangenome\n",
    "    \"\"\"\n",
    "\n",
    "    translate_dict = {\"Pangenome\": {\"name\": pangenome.name, \"taxid\": pangenome.taxid,\n",
    "                                  \"Family\": [],\n",
    "                                  \"Partition\": [],\n",
    "                                  \"Module\": [],\n",
    "                                  \"RGP\": write_rgp(parent=pangenome),\n",
    "                                  \"Spot\": [], \"Genome\": []}}\n",
    "    write_families(pangenome, translate_dict)\n",
    "    write_organisms(pangenome, translate_dict)\n",
    "    write_spot(pangenome, translate_dict)\n",
    "    write_modules(pangenome, translate_dict)\n",
    "    return translate_dict"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Loader configuration\n",
    "To export pangenomes into our graph database we are using the package employed by the CovidGraph framework."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "from multiprocessing import Lock\n",
    "\n",
    "from py2neo import Node\n",
    "# installed librairies\n",
    "from dict2graph import Dict2graph\n",
    "\n",
    "\n",
    "def custom_post_func(node: Node):\n",
    "    if node is not None and node.__primarylabel__ == \"Gene\":\n",
    "        del node[\"tmp_id\"]\n",
    "    return node\n",
    "\n",
    "\n",
    "class PangenomeLoader:\n",
    "\n",
    "    def __init__(self, pangenome_name: str, pangenome_data: dict, lock: Lock, batch_size: int = 1000):\n",
    "        self.name = pangenome_name\n",
    "        self.lock = lock\n",
    "        self.data = pangenome_data\n",
    "        self.batch_size = batch_size\n",
    "        self._build_loader()\n",
    "\n",
    "    def load(self, graph: Graph):\n",
    "        assert self.lock is not None, \"Lock not Initialized\"\n",
    "        try:\n",
    "            with self.lock:\n",
    "                logging.getLogger().debug(\"parse\")\n",
    "                self.loader.parse(self.data)\n",
    "                logging.getLogger().debug(\"index\")\n",
    "                self.loader.create_indexes(graph)\n",
    "                logging.getLogger().debug(\"merge\")\n",
    "                self.loader.merge(graph)\n",
    "        except Exception as error:\n",
    "            raise Exception(f\"Load to Neo4j failed because : {error}\")\n",
    "\n",
    "    def _build_loader(self):\n",
    "        d2g = Dict2graph()\n",
    "        d2g.config_dict_primarykey_generated_hashed_attrs_by_label = {\n",
    "            \"Pangenome\": 'AllAttributes',  # Random id\n",
    "            \"Family\": [\"name\"],\n",
    "            \"Partition\": 'AllAttributes',\n",
    "            \"Gene\": 'AllAttributes',\n",
    "            \"Module\": \"InnerContent\",\n",
    "            \"Spot\": \"AllContent\",\n",
    "            \"RGP\": \"InnerContent\",\n",
    "            \"Genome\": \"InnerContent\",\n",
    "            \"Contig\": \"AllContent\"\n",
    "        }\n",
    "        d2g.config_str_primarykey_generated_attr_name = \"hash_id\"\n",
    "        d2g.config_list_blocklist_collection_hubs = [\n",
    "            \"PangenomeCollection\",\n",
    "            \"FamilyCollection\",\n",
    "            \"PartitionCollection\",\n",
    "            \"GeneCollection\",\n",
    "            \"NeighborCollection\",\n",
    "            \"ModuleCollection\",\n",
    "            \"SpotCollection\",\n",
    "            \"RGPCollection\",\n",
    "            \"GenomeCollection\",\n",
    "            \"ContigCollection\",\n",
    "        ]\n",
    "        d2g.config_dict_node_prop_to_rel_prop = {\"Family\": {\"weight\": [\"NEIGHBOR\"]}} #,\n",
    "                                                 # \"Shell\": {\"weight\": [\"NEIGHBOR\"]},\n",
    "                                                 # \"Cloud\": {\"weight\": [\"NEIGHBOR\"]}}  # ,  \"partition\": [\"IN_MODULE\"]}}\n",
    "        d2g.config_dict_primarykey_attr_by_label = {\"Family\": [\"name\"],\n",
    "                                                    \"Gene\": [\"name\"],\n",
    "                                                    \"Partition\": [\"partition\"]}\n",
    "        d2g.config_dict_reltype_override = {\"PANGENOME_HAS_FAMILY\": \"IS_IN_PANGENOME\",\n",
    "                                            \"FAMILY_HAS_GENE\": \"IS_IN_FAMILY\",\n",
    "                                            \"FAMILY_HAS_PARTITION\": \"HAS_PARTITION\",\n",
    "                                            \"FAMILY_HAS_FAMILY\": \"NEIGHBOR\",\n",
    "                                            \"MODULE_HAS_FAMILY\": \"IS_IN_MODULE\",\n",
    "                                            \"SPOT_HAS_RGP\": \"IS_IN_SPOT\",\n",
    "                                            \"RGP_HAS_GENE\": \"IS_IN_RGP\",\n",
    "                                            \"GENOME_HAS_CONTIG\": \"IS_IN_GENOME\",\n",
    "                                            \"CONTIG_HAS_GENE\": \"IS_IN_CONTIG\"}\n",
    "        d2g.config_list_blocklist_reltypes = [\"PANGENOME_HAS_MODULE\",\n",
    "                                              \"PANGENOME_HAS_RGP\",\n",
    "                                              \"PANGENOME_HAS_SPOT\",\n",
    "                                              \"PANGENOME_HAS_GENOME\"]\n",
    "        d2g.config_bool_capitalize_labels = False\n",
    "        d2g.config_func_node_post_modifier = custom_post_func\n",
    "        d2g.config_graphio_batch_size = self.batch_size\n",
    "        self.loader = d2g"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Export to Graph Database\n",
    "### Export pangenomes"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin pangenomes load\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 664001/664001 [00:01<00:00, 338876.72gene/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████| 1070265/1070265 [00:03<00:00, 283936.29gene/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 137/137 [00:11<00:00, 12.23organism/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████| 651827/651827 [00:03<00:00, 173287.53gene family/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████| 22953/22953 [00:00<00:00, 84615.41gene family/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 639082/639082 [00:04<00:00, 136840.02contig adjacency/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 98368/98368 [00:00<00:00, 245194.61region/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 3630/3630 [00:00<00:00, 15106.72spot/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 3404/3404 [00:00<00:00, 231685.88module/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 110/110 [00:00<00:00, 110588.07gene family/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 284/284 [00:19<00:00, 14.42organism/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████| 1044515/1044515 [00:05<00:00, 204535.42gene family/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████| 14400/14400 [00:00<00:00, 64633.54gene family/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 1043422/1043422 [00:06<00:00, 171802.73contig adjacency/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████| 262116/262116 [00:00<00:00, 374185.55region/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 9332/9332 [00:02<00:00, 3841.70spot/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 3790/3790 [00:00<00:00, 275759.15module/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████| 92/92 [00:00<00:00, 143341.74gene family/s]\n",
      "index rels: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 9/9 [00:00<00:00, 20.98rels/s]\n",
      "index nodes: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 9/9 [00:00<00:00, 263.15node/s]\n",
      "merge nodes: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 9/9 [00:56<00:00,  6.31s/node]\n",
      "merge rels: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 9/9 [01:50<00:00, 12.33s/rels]\n",
      "index rels: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 9/9 [00:00<00:00, 291.98rels/s]\n",
      "index nodes: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 9/9 [00:00<00:00, 558.50node/s]\n",
      "merge nodes: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 9/9 [01:37<00:00, 10.86s/node]\n",
      "merge rels: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 9/9 [03:11<00:00, 21.26s/rels]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [25:41<00:00, 770.94s/pangenome]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All pangenomes loaded in : 0:25:42.722878\n"
     ]
    }
   ],
   "source": [
    "# default libraries\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from multiprocessing import Manager, Lock\n",
    "from time import time\n",
    "from datetime import timedelta\n",
    "\n",
    "# local librairies\n",
    "from script.python import Pangenome\n",
    "from script.python.export import give_gene_tmp_id\n",
    "from script.python.utils import check_pangenome_info\n",
    "\n",
    "\n",
    "db_loading_lock: Lock = None\n",
    "\n",
    "\n",
    "def init_db_lock(lock: Lock):\n",
    "    global db_loading_lock\n",
    "    if db_loading_lock is None:\n",
    "        db_loading_lock = lock\n",
    "\n",
    "\n",
    "def load_pangenome(pangenome_name, pangenome_info, batch_size: int = 1000):\n",
    "    \"\"\"\n",
    "\n",
    "    :param pangenome_name:\n",
    "    :param pangenome_info:\n",
    "    :param batch_size:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    logging.getLogger(f\"Add {pangenome_name} to load list\")\n",
    "    pangenome = Pangenome(name=pangenome_name, taxid=pangenome_info[\"taxid\"])\n",
    "    pangenome.add_file(pangenome_info[\"path\"])\n",
    "    check_pangenome_info(pangenome, need_annotations=True, need_families=True, need_graph=True,\n",
    "                         need_rgp=True, need_spots=True, need_modules=True, need_anntation_fam=True,\n",
    "                         disable_bar=False)\n",
    "    give_gene_tmp_id(pangenome)\n",
    "    data = create_dict(pangenome)\n",
    "    loader = PangenomeLoader(pangenome_name, data, db_loading_lock, batch_size=batch_size)\n",
    "    loader.load(graph)\n",
    "\n",
    "\n",
    "def load_pangenome_mp(pangenomes: dict, cpu: int = 1, batch_size: int = 1000):\n",
    "    \"\"\"\n",
    "\n",
    "    :param pangenomes:\n",
    "    :param cpu:\n",
    "    :param batch_size:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    manager = Manager()\n",
    "    lock = manager.Lock()\n",
    "    with ProcessPoolExecutor(max_workers=cpu, initializer=init_db_lock, initargs=(lock,)) as executor:\n",
    "        list(tqdm(executor.map(load_pangenome, pangenomes.keys(), pangenomes.values(),\n",
    "                               [batch_size] * len(pangenomes)),\n",
    "                  total=len(pangenomes), unit='pangenome'))\n",
    "\n",
    "print(\"Begin pangenomes load\")\n",
    "begin_load_time = time()\n",
    "load_pangenome_mp(pangenomes, CPU, BATCH_SIZE)\n",
    "load_time = time() - begin_load_time\n",
    "print(f\"All pangenomes loaded in : {timedelta(seconds=load_time)}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Export similarities"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bengin load of similarities...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [07:06<00:00, 426.45s/similarities_batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All similarities loaded in : 0:07:06.837499\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# installed librairies\n",
    "from graphio import RelationshipSet\n",
    "import pandas as pd\n",
    "\n",
    "def load_similarities(tsv: Path, batch_size: int = 1000):\n",
    "    df = pd.read_csv(filepath_or_buffer=tsv, sep=\"\\t\", header=None,\n",
    "                     names=[\"Family_1\", \"Family_2\", \"identity\", \"covery\"])\n",
    "\n",
    "    is_similar_list = []\n",
    "    is_similar_to = RelationshipSet('IS_SIMILAR', ['Family'], ['Family'], ['name'], ['name'])\n",
    "    chunk_size = batch_size * 10\n",
    "    for row in df.iterrows():\n",
    "        if len(is_similar_to.relationships) >= chunk_size:\n",
    "            is_similar_list.append(is_similar_to)\n",
    "            is_similar_to = RelationshipSet('IS_SIMILAR', ['Family'], ['Family'], ['name'], ['name'])\n",
    "        is_similar_to.add_relationship(start_node_properties={\"name\": row[1]['Family_1']},\n",
    "                                       end_node_properties={\"name\": row[1]['Family_2']},\n",
    "                                       properties={\"identity\": row[1]['identity'],\n",
    "                                                   \"coverage\": row[1]['covery']})\n",
    "    is_similar_list.append(is_similar_to)\n",
    "    for sim in tqdm(is_similar_list, unit=\"similarities_batch\", total=len(is_similar_list)):\n",
    "        sim.merge(graph=graph, batch_size=batch_size)\n",
    "\n",
    "print(\"Bengin load of similarities...\")\n",
    "begin_sim_time = time()\n",
    "load_similarities(SIMILARITIES, BATCH_SIZE)\n",
    "sim_time = time() - begin_sim_time\n",
    "print(f\"All similarities loaded in : {timedelta(seconds=sim_time)}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Invert edges"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invert edges...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [03:20<00:00, 28.62s/label]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All edges inverted in : 0:03:20.377133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from script.python.export import invert_edges_query\n",
    "\n",
    "\n",
    "def invert_edges(edge_label: str):\n",
    "    query = invert_edges_query(edge_label)\n",
    "    try:\n",
    "        graph.run(query)\n",
    "    except Exception as errror:\n",
    "        raise Exception(f\"Invert edges failed because : {errror}\")\n",
    "\n",
    "print(\"Invert edges...\")\n",
    "begin_invert_time = time()\n",
    "labels2invert = [\"IS_IN_PANGENOME\", \"IS_IN_MODULE\", \"IS_IN_FAMILY\", \"IS_IN_CONTIG\",\n",
    "                 \"IS_IN_GENOME\", \"IS_IN_SPOT\", \"IS_IN_RGP\"]\n",
    "for edge_label in tqdm(labels2invert, unit='label'):\n",
    "    logging.getLogger().debug(f\"Invert: {edge_label}\")\n",
    "    invert_edges(edge_label)\n",
    "invert_time = time() - begin_invert_time\n",
    "print(f\"All edges inverted in : {timedelta(seconds=invert_time)}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Request workflow"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CALL apoc.warmup.run()\n",
      "MATCH (p:Pangenome)<-[:IS_IN_PANGENOME]-(f:Family) WHERE f.annotation IS NOT NULL RETURN p.name, count(f)\n",
      "CALL db.clearQueryCaches()\n",
      "CALL apoc.warmup.run()\n",
      "MATCH (p:Pangenome)<-[:IS_IN_PANGENOME]-(f:Family) WHERE f.annotation IS NOT NULL RETURN p.name, count(f)\n",
      "CALL db.clearQueryCaches()\n",
      "CALL apoc.warmup.run()\n",
      "MATCH (p:Pangenome)<-[:IS_IN_PANGENOME]-(f:Family) WHERE f.annotation IS NOT NULL RETURN p.name, count(f)\n",
      "CALL db.clearQueryCaches()\n",
      "CALL apoc.warmup.run()\n",
      "MATCH (p:Pangenome)<-[:IS_IN_PANGENOME]-(f:Family) WHERE f.annotation IS NOT NULL RETURN p.name, count(f)\n",
      "CALL db.clearQueryCaches()\n",
      "CALL apoc.warmup.run()\n",
      "MATCH (p:Pangenome)<-[:IS_IN_PANGENOME]-(f:Family) WHERE f.annotation IS NOT NULL RETURN p.name, count(f)\n",
      "CALL db.clearQueryCaches()\n",
      "CALL apoc.warmup.run()\n",
      "MATCH (p:Pangenome)<-[:IS_IN_PANGENOME]-(f:Family) WHERE f.annotation IS NOT NULL RETURN p.name, count(f)\n",
      "CALL db.clearQueryCaches()\n",
      "CALL apoc.warmup.run()\n",
      "MATCH (p:Pangenome)<-[:IS_IN_PANGENOME]-(f:Family) WHERE f.annotation IS NOT NULL RETURN p.name, count(f)\n",
      "CALL db.clearQueryCaches()\n",
      "CALL apoc.warmup.run()\n",
      "MATCH (p:Pangenome)<-[:IS_IN_PANGENOME]-(f:Family) WHERE f.annotation IS NOT NULL RETURN p.name, count(f)\n",
      "CALL db.clearQueryCaches()\n",
      "CALL apoc.warmup.run()\n",
      "MATCH (p:Pangenome)<-[:IS_IN_PANGENOME]-(f:Family) WHERE f.annotation IS NOT NULL RETURN p.name, count(f)\n",
      "CALL db.clearQueryCaches()\n",
      "CALL apoc.warmup.run()\n",
      "MATCH (p:Pangenome)<-[:IS_IN_PANGENOME]-(f:Family) WHERE f.annotation IS NOT NULL RETURN p.name, count(f)\n",
      "CALL db.clearQueryCaches()\n",
      "CALL apoc.warmup.run()\n",
      "MATCH (p:Pangenome)<-[:IS_IN_PANGENOME]-(f:Family) WHERE f.annotation IS NOT NULL RETURN p.name, count(f)\n",
      "CALL db.clearQueryCaches()\n",
      "CALL apoc.warmup.run()\n",
      "MATCH (p:Pangenome)<-[:IS_IN_PANGENOME]-(f:Family) WHERE f.annotation IS NOT NULL RETURN p.name, count(f)\n",
      "CALL db.clearQueryCaches()\n",
      "CALL apoc.warmup.run()\n",
      "MATCH (p:Pangenome)<-[:IS_IN_PANGENOME]-(f:Family) WHERE f.annotation IS NOT NULL RETURN p.name, count(f)\n",
      "CALL db.clearQueryCaches()\n",
      "CALL apoc.warmup.run()\n",
      "MATCH (p:Pangenome)<-[:IS_IN_PANGENOME]-(f:Family) WHERE f.annotation IS NOT NULL RETURN p.name, count(f)\n",
      "CALL db.clearQueryCaches()\n",
      "CALL apoc.warmup.run()\n",
      "MATCH (p:Pangenome)<-[:IS_IN_PANGENOME]-(f:Family) WHERE f.annotation IS NOT NULL RETURN p.name, count(f)\n",
      "CALL db.clearQueryCaches()\n",
      "CALL apoc.warmup.run()\n",
      "MATCH (p:Pangenome)<-[:IS_IN_PANGENOME]-(f:Family) WHERE f.annotation IS NOT NULL RETURN p.name, count(f)\n",
      "CALL db.clearQueryCaches()\n",
      "CALL apoc.warmup.run()\n",
      "MATCH (p:Pangenome)<-[:IS_IN_PANGENOME]-(f:Family) WHERE f.annotation IS NOT NULL RETURN p.name, count(f)\n",
      "CALL db.clearQueryCaches()\n",
      "CALL apoc.warmup.run()\n",
      "MATCH (p:Pangenome)<-[:IS_IN_PANGENOME]-(f:Family) WHERE f.annotation IS NOT NULL RETURN p.name, count(f)\n",
      "CALL db.clearQueryCaches()\n",
      "CALL apoc.warmup.run()\n",
      "MATCH (p:Pangenome)<-[:IS_IN_PANGENOME]-(f:Family) WHERE f.annotation IS NOT NULL RETURN p.name, count(f)\n",
      "CALL db.clearQueryCaches()\n",
      "CALL apoc.warmup.run()\n",
      "MATCH (p:Pangenome)<-[:IS_IN_PANGENOME]-(f:Family) WHERE f.annotation IS NOT NULL RETURN p.name, count(f)\n",
      "CALL db.clearQueryCaches()\n",
      "CALL apoc.warmup.run()\n",
      "MATCH (p:Pangenome)<-[:IS_IN_PANGENOME]-(f:Family)<-[:IS_IN_FAMILY]-(:Gene)-[:IS_IN_RGP]->(r:RGP)-[:IS_IN_SPOT]-(s:Spot) WHERE f.annotation IS NOT NULL WITH f, p, s, count(DISTINCT r) AS cnt ORDER BY cnt DESC RETURN f.name, p.name, cnt, s.name\n",
      "CALL db.clearQueryCaches()\n",
      "CALL apoc.warmup.run()\n",
      "MATCH (p:Pangenome)<-[:IS_IN_PANGENOME]-(f:Family)<-[:IS_IN_FAMILY]-(:Gene)-[:IS_IN_RGP]->(r:RGP)-[:IS_IN_SPOT]-(s:Spot) WHERE f.annotation IS NOT NULL WITH f, p, s, count(DISTINCT r) AS cnt ORDER BY cnt DESC RETURN f.name, p.name, cnt, s.name\n",
      "CALL db.clearQueryCaches()\n",
      "CALL apoc.warmup.run()\n",
      "MATCH (p:Pangenome)<-[:IS_IN_PANGENOME]-(f:Family)<-[:IS_IN_FAMILY]-(:Gene)-[:IS_IN_RGP]->(r:RGP)-[:IS_IN_SPOT]-(s:Spot) WHERE f.annotation IS NOT NULL WITH f, p, s, count(DISTINCT r) AS cnt ORDER BY cnt DESC RETURN f.name, p.name, cnt, s.name\n",
      "CALL db.clearQueryCaches()\n",
      "CALL apoc.warmup.run()\n",
      "MATCH (p:Pangenome)<-[:IS_IN_PANGENOME]-(f:Family)<-[:IS_IN_FAMILY]-(:Gene)-[:IS_IN_RGP]->(r:RGP)-[:IS_IN_SPOT]-(s:Spot) WHERE f.annotation IS NOT NULL WITH f, p, s, count(DISTINCT r) AS cnt ORDER BY cnt DESC RETURN f.name, p.name, cnt, s.name\n",
      "CALL db.clearQueryCaches()\n",
      "CALL apoc.warmup.run()\n",
      "MATCH (p:Pangenome)<-[:IS_IN_PANGENOME]-(f:Family)<-[:IS_IN_FAMILY]-(:Gene)-[:IS_IN_RGP]->(r:RGP)-[:IS_IN_SPOT]-(s:Spot) WHERE f.annotation IS NOT NULL WITH f, p, s, count(DISTINCT r) AS cnt ORDER BY cnt DESC RETURN f.name, p.name, cnt, s.name\n",
      "CALL db.clearQueryCaches()\n",
      "CALL apoc.warmup.run()\n",
      "MATCH (p:Pangenome)<-[:IS_IN_PANGENOME]-(f:Family)<-[:IS_IN_FAMILY]-(:Gene)-[:IS_IN_RGP]->(r:RGP)-[:IS_IN_SPOT]-(s:Spot) WHERE f.annotation IS NOT NULL WITH f, p, s, count(DISTINCT r) AS cnt ORDER BY cnt DESC RETURN f.name, p.name, cnt, s.name\n",
      "CALL db.clearQueryCaches()\n",
      "CALL apoc.warmup.run()\n",
      "MATCH (p:Pangenome)<-[:IS_IN_PANGENOME]-(f:Family)<-[:IS_IN_FAMILY]-(:Gene)-[:IS_IN_RGP]->(r:RGP)-[:IS_IN_SPOT]-(s:Spot) WHERE f.annotation IS NOT NULL WITH f, p, s, count(DISTINCT r) AS cnt ORDER BY cnt DESC RETURN f.name, p.name, cnt, s.name\n",
      "CALL db.clearQueryCaches()\n",
      "CALL apoc.warmup.run()\n",
      "MATCH (p:Pangenome)<-[:IS_IN_PANGENOME]-(f:Family)<-[:IS_IN_FAMILY]-(:Gene)-[:IS_IN_RGP]->(r:RGP)-[:IS_IN_SPOT]-(s:Spot) WHERE f.annotation IS NOT NULL WITH f, p, s, count(DISTINCT r) AS cnt ORDER BY cnt DESC RETURN f.name, p.name, cnt, s.name\n",
      "CALL db.clearQueryCaches()\n",
      "CALL apoc.warmup.run()\n",
      "MATCH (p:Pangenome)<-[:IS_IN_PANGENOME]-(f:Family)<-[:IS_IN_FAMILY]-(:Gene)-[:IS_IN_RGP]->(r:RGP)-[:IS_IN_SPOT]-(s:Spot) WHERE f.annotation IS NOT NULL WITH f, p, s, count(DISTINCT r) AS cnt ORDER BY cnt DESC RETURN f.name, p.name, cnt, s.name\n",
      "CALL db.clearQueryCaches()\n",
      "CALL apoc.warmup.run()\n",
      "MATCH (p:Pangenome)<-[:IS_IN_PANGENOME]-(f:Family)<-[:IS_IN_FAMILY]-(:Gene)-[:IS_IN_RGP]->(r:RGP)-[:IS_IN_SPOT]-(s:Spot) WHERE f.annotation IS NOT NULL WITH f, p, s, count(DISTINCT r) AS cnt ORDER BY cnt DESC RETURN f.name, p.name, cnt, s.name\n",
      "CALL db.clearQueryCaches()\n",
      "CALL apoc.warmup.run()\n",
      "MATCH (p:Pangenome)<-[:IS_IN_PANGENOME]-(f:Family)<-[:IS_IN_FAMILY]-(g:Gene)-[:IS_IN_RGP]->(r:RGP) WITH p, count(DISTINCT r) AS cnt ORDER BY cnt DESC LIMIT 10 RETURN p.name, cnt\n",
      "CALL db.clearQueryCaches()\n",
      "CALL apoc.warmup.run()\n",
      "MATCH (p:Pangenome)<-[:IS_IN_PANGENOME]-(f:Family)<-[:IS_IN_FAMILY]-(g:Gene)-[:IS_IN_RGP]->(r:RGP) WITH p, count(DISTINCT r) AS cnt ORDER BY cnt DESC LIMIT 10 RETURN p.name, cnt\n",
      "CALL db.clearQueryCaches()\n",
      "CALL apoc.warmup.run()\n",
      "MATCH (p:Pangenome)<-[:IS_IN_PANGENOME]-(f:Family)<-[:IS_IN_FAMILY]-(g:Gene)-[:IS_IN_RGP]->(r:RGP) WITH p, count(DISTINCT r) AS cnt ORDER BY cnt DESC LIMIT 10 RETURN p.name, cnt\n",
      "CALL db.clearQueryCaches()\n",
      "CALL apoc.warmup.run()\n",
      "MATCH (p:Pangenome)<-[:IS_IN_PANGENOME]-(f:Family)<-[:IS_IN_FAMILY]-(g:Gene)-[:IS_IN_RGP]->(r:RGP) WITH p, count(DISTINCT r) AS cnt ORDER BY cnt DESC LIMIT 10 RETURN p.name, cnt\n",
      "CALL db.clearQueryCaches()\n",
      "CALL apoc.warmup.run()\n",
      "MATCH (p:Pangenome)<-[:IS_IN_PANGENOME]-(f:Family)<-[:IS_IN_FAMILY]-(g:Gene)-[:IS_IN_RGP]->(r:RGP) WITH p, count(DISTINCT r) AS cnt ORDER BY cnt DESC LIMIT 10 RETURN p.name, cnt\n",
      "CALL db.clearQueryCaches()\n",
      "CALL apoc.warmup.run()\n",
      "MATCH (p:Pangenome)<-[:IS_IN_PANGENOME]-(f:Family)<-[:IS_IN_FAMILY]-(g:Gene)-[:IS_IN_RGP]->(r:RGP) WITH p, count(DISTINCT r) AS cnt ORDER BY cnt DESC LIMIT 10 RETURN p.name, cnt\n",
      "CALL db.clearQueryCaches()\n",
      "CALL apoc.warmup.run()\n",
      "MATCH (p:Pangenome)<-[:IS_IN_PANGENOME]-(f:Family)<-[:IS_IN_FAMILY]-(g:Gene)-[:IS_IN_RGP]->(r:RGP) WITH p, count(DISTINCT r) AS cnt ORDER BY cnt DESC LIMIT 10 RETURN p.name, cnt\n",
      "CALL db.clearQueryCaches()\n",
      "CALL apoc.warmup.run()\n",
      "MATCH (p:Pangenome)<-[:IS_IN_PANGENOME]-(f:Family)<-[:IS_IN_FAMILY]-(g:Gene)-[:IS_IN_RGP]->(r:RGP) WITH p, count(DISTINCT r) AS cnt ORDER BY cnt DESC LIMIT 10 RETURN p.name, cnt\n",
      "CALL db.clearQueryCaches()\n",
      "CALL apoc.warmup.run()\n",
      "MATCH (p:Pangenome)<-[:IS_IN_PANGENOME]-(f:Family)<-[:IS_IN_FAMILY]-(g:Gene)-[:IS_IN_RGP]->(r:RGP) WITH p, count(DISTINCT r) AS cnt ORDER BY cnt DESC LIMIT 10 RETURN p.name, cnt\n",
      "CALL db.clearQueryCaches()\n",
      "CALL apoc.warmup.run()\n",
      "MATCH (p:Pangenome)<-[:IS_IN_PANGENOME]-(f:Family)<-[:IS_IN_FAMILY]-(g:Gene)-[:IS_IN_RGP]->(r:RGP) WITH p, count(DISTINCT r) AS cnt ORDER BY cnt DESC LIMIT 10 RETURN p.name, cnt\n",
      "CALL db.clearQueryCaches()\n",
      "CALL apoc.warmup.run()\n",
      "MATCH (p:Pangenome)<-[:IS_IN_PANGENOME]-(f:Family)-[:IS_IN_MODULE]->(m:Module) WITH p, count(DISTINCT f) AS cnt ORDER BY cnt DESC LIMIT 10 RETURN p.name, cnt\n",
      "CALL db.clearQueryCaches()\n",
      "CALL apoc.warmup.run()\n",
      "MATCH (p:Pangenome)<-[:IS_IN_PANGENOME]-(f:Family)-[:IS_IN_MODULE]->(m:Module) WITH p, count(DISTINCT f) AS cnt ORDER BY cnt DESC LIMIT 10 RETURN p.name, cnt\n",
      "CALL db.clearQueryCaches()\n",
      "CALL apoc.warmup.run()\n",
      "MATCH (p:Pangenome)<-[:IS_IN_PANGENOME]-(f:Family)-[:IS_IN_MODULE]->(m:Module) WITH p, count(DISTINCT f) AS cnt ORDER BY cnt DESC LIMIT 10 RETURN p.name, cnt\n",
      "CALL db.clearQueryCaches()\n",
      "CALL apoc.warmup.run()\n",
      "MATCH (p:Pangenome)<-[:IS_IN_PANGENOME]-(f:Family)-[:IS_IN_MODULE]->(m:Module) WITH p, count(DISTINCT f) AS cnt ORDER BY cnt DESC LIMIT 10 RETURN p.name, cnt\n",
      "CALL db.clearQueryCaches()\n",
      "CALL apoc.warmup.run()\n",
      "MATCH (p:Pangenome)<-[:IS_IN_PANGENOME]-(f:Family)-[:IS_IN_MODULE]->(m:Module) WITH p, count(DISTINCT f) AS cnt ORDER BY cnt DESC LIMIT 10 RETURN p.name, cnt\n",
      "CALL db.clearQueryCaches()\n",
      "CALL apoc.warmup.run()\n",
      "MATCH (p:Pangenome)<-[:IS_IN_PANGENOME]-(f:Family)-[:IS_IN_MODULE]->(m:Module) WITH p, count(DISTINCT f) AS cnt ORDER BY cnt DESC LIMIT 10 RETURN p.name, cnt\n",
      "CALL db.clearQueryCaches()\n",
      "CALL apoc.warmup.run()\n",
      "MATCH (p:Pangenome)<-[:IS_IN_PANGENOME]-(f:Family)-[:IS_IN_MODULE]->(m:Module) WITH p, count(DISTINCT f) AS cnt ORDER BY cnt DESC LIMIT 10 RETURN p.name, cnt\n",
      "CALL db.clearQueryCaches()\n",
      "CALL apoc.warmup.run()\n",
      "MATCH (p:Pangenome)<-[:IS_IN_PANGENOME]-(f:Family)-[:IS_IN_MODULE]->(m:Module) WITH p, count(DISTINCT f) AS cnt ORDER BY cnt DESC LIMIT 10 RETURN p.name, cnt\n",
      "CALL db.clearQueryCaches()\n",
      "CALL apoc.warmup.run()\n",
      "MATCH (p:Pangenome)<-[:IS_IN_PANGENOME]-(f:Family)-[:IS_IN_MODULE]->(m:Module) WITH p, count(DISTINCT f) AS cnt ORDER BY cnt DESC LIMIT 10 RETURN p.name, cnt\n",
      "CALL db.clearQueryCaches()\n",
      "CALL apoc.warmup.run()\n",
      "MATCH (p:Pangenome)<-[:IS_IN_PANGENOME]-(f:Family)-[:IS_IN_MODULE]->(m:Module) WITH p, count(DISTINCT f) AS cnt ORDER BY cnt DESC LIMIT 10 RETURN p.name, cnt\n",
      "CALL db.clearQueryCaches()\n",
      "CALL apoc.warmup.run()\n",
      "MATCH (p:Pangenome)<-[:IS_IN_PANGENOME]-(f:Family)-[:IS_IN_MODULE]->(m:Module) WITH p, count(DISTINCT m) AS cnt ORDER BY cnt DESC LIMIT 10 RETURN p.name, cnt\n",
      "CALL db.clearQueryCaches()\n",
      "CALL apoc.warmup.run()\n",
      "MATCH (p:Pangenome)<-[:IS_IN_PANGENOME]-(f:Family)-[:IS_IN_MODULE]->(m:Module) WITH p, count(DISTINCT m) AS cnt ORDER BY cnt DESC LIMIT 10 RETURN p.name, cnt\n",
      "CALL db.clearQueryCaches()\n",
      "CALL apoc.warmup.run()\n",
      "MATCH (p:Pangenome)<-[:IS_IN_PANGENOME]-(f:Family)-[:IS_IN_MODULE]->(m:Module) WITH p, count(DISTINCT m) AS cnt ORDER BY cnt DESC LIMIT 10 RETURN p.name, cnt\n",
      "CALL db.clearQueryCaches()\n",
      "CALL apoc.warmup.run()\n",
      "MATCH (p:Pangenome)<-[:IS_IN_PANGENOME]-(f:Family)-[:IS_IN_MODULE]->(m:Module) WITH p, count(DISTINCT m) AS cnt ORDER BY cnt DESC LIMIT 10 RETURN p.name, cnt\n",
      "CALL db.clearQueryCaches()\n",
      "CALL apoc.warmup.run()\n",
      "MATCH (p:Pangenome)<-[:IS_IN_PANGENOME]-(f:Family)-[:IS_IN_MODULE]->(m:Module) WITH p, count(DISTINCT m) AS cnt ORDER BY cnt DESC LIMIT 10 RETURN p.name, cnt\n",
      "CALL db.clearQueryCaches()\n",
      "CALL apoc.warmup.run()\n",
      "MATCH (p:Pangenome)<-[:IS_IN_PANGENOME]-(f:Family)-[:IS_IN_MODULE]->(m:Module) WITH p, count(DISTINCT m) AS cnt ORDER BY cnt DESC LIMIT 10 RETURN p.name, cnt\n",
      "CALL db.clearQueryCaches()\n",
      "CALL apoc.warmup.run()\n",
      "MATCH (p:Pangenome)<-[:IS_IN_PANGENOME]-(f:Family)-[:IS_IN_MODULE]->(m:Module) WITH p, count(DISTINCT m) AS cnt ORDER BY cnt DESC LIMIT 10 RETURN p.name, cnt\n",
      "CALL db.clearQueryCaches()\n",
      "CALL apoc.warmup.run()\n",
      "MATCH (p:Pangenome)<-[:IS_IN_PANGENOME]-(f:Family)-[:IS_IN_MODULE]->(m:Module) WITH p, count(DISTINCT m) AS cnt ORDER BY cnt DESC LIMIT 10 RETURN p.name, cnt\n",
      "CALL db.clearQueryCaches()\n",
      "CALL apoc.warmup.run()\n",
      "MATCH (p:Pangenome)<-[:IS_IN_PANGENOME]-(f:Family)-[:IS_IN_MODULE]->(m:Module) WITH p, count(DISTINCT m) AS cnt ORDER BY cnt DESC LIMIT 10 RETURN p.name, cnt\n",
      "CALL db.clearQueryCaches()\n",
      "CALL apoc.warmup.run()\n",
      "MATCH (p:Pangenome)<-[:IS_IN_PANGENOME]-(f:Family)-[:IS_IN_MODULE]->(m:Module) WITH p, count(DISTINCT m) AS cnt ORDER BY cnt DESC LIMIT 10 RETURN p.name, cnt\n",
      "CALL db.clearQueryCaches()\n",
      "CALL apoc.warmup.run()\n",
      "MATCH (p1:Pangenome)<-[:IS_IN_PANGENOME]-(f1:Family)-[:HAS_PARTITION]->(s1:Partition) WITH p1, f1, s1 MATCH (f1)-[IS_SIMILAR]-(f2:Family) -[:HAS_PARTITION]->(s2) WITH p1, f1, s1, s2, f2 MATCH (p2:Pangenome)<-[IS_IN_PANGENOME]-(f2) WHERE p1.name <> p2.name RETURN p1.name, f1.name, s1.partition, p2.name, f2.name, s2.partition\n",
      "CALL db.clearQueryCaches()\n",
      "CALL apoc.warmup.run()\n",
      "MATCH (p1:Pangenome)<-[:IS_IN_PANGENOME]-(f1:Family)-[:HAS_PARTITION]->(s1:Partition) WITH p1, f1, s1 MATCH (f1)-[IS_SIMILAR]-(f2:Family) -[:HAS_PARTITION]->(s2) WITH p1, f1, s1, s2, f2 MATCH (p2:Pangenome)<-[IS_IN_PANGENOME]-(f2) WHERE p1.name <> p2.name RETURN p1.name, f1.name, s1.partition, p2.name, f2.name, s2.partition\n",
      "CALL db.clearQueryCaches()\n",
      "CALL apoc.warmup.run()\n",
      "MATCH (p1:Pangenome)<-[:IS_IN_PANGENOME]-(f1:Family)-[:HAS_PARTITION]->(s1:Partition) WITH p1, f1, s1 MATCH (f1)-[IS_SIMILAR]-(f2:Family) -[:HAS_PARTITION]->(s2) WITH p1, f1, s1, s2, f2 MATCH (p2:Pangenome)<-[IS_IN_PANGENOME]-(f2) WHERE p1.name <> p2.name RETURN p1.name, f1.name, s1.partition, p2.name, f2.name, s2.partition\n",
      "CALL db.clearQueryCaches()\n",
      "CALL apoc.warmup.run()\n",
      "MATCH (p1:Pangenome)<-[:IS_IN_PANGENOME]-(f1:Family)-[:HAS_PARTITION]->(s1:Partition) WITH p1, f1, s1 MATCH (f1)-[IS_SIMILAR]-(f2:Family) -[:HAS_PARTITION]->(s2) WITH p1, f1, s1, s2, f2 MATCH (p2:Pangenome)<-[IS_IN_PANGENOME]-(f2) WHERE p1.name <> p2.name RETURN p1.name, f1.name, s1.partition, p2.name, f2.name, s2.partition\n",
      "CALL db.clearQueryCaches()\n",
      "CALL apoc.warmup.run()\n",
      "MATCH (p1:Pangenome)<-[:IS_IN_PANGENOME]-(f1:Family)-[:HAS_PARTITION]->(s1:Partition) WITH p1, f1, s1 MATCH (f1)-[IS_SIMILAR]-(f2:Family) -[:HAS_PARTITION]->(s2) WITH p1, f1, s1, s2, f2 MATCH (p2:Pangenome)<-[IS_IN_PANGENOME]-(f2) WHERE p1.name <> p2.name RETURN p1.name, f1.name, s1.partition, p2.name, f2.name, s2.partition\n",
      "CALL db.clearQueryCaches()\n",
      "CALL apoc.warmup.run()\n",
      "MATCH (p1:Pangenome)<-[:IS_IN_PANGENOME]-(f1:Family)-[:HAS_PARTITION]->(s1:Partition) WITH p1, f1, s1 MATCH (f1)-[IS_SIMILAR]-(f2:Family) -[:HAS_PARTITION]->(s2) WITH p1, f1, s1, s2, f2 MATCH (p2:Pangenome)<-[IS_IN_PANGENOME]-(f2) WHERE p1.name <> p2.name RETURN p1.name, f1.name, s1.partition, p2.name, f2.name, s2.partition\n",
      "CALL db.clearQueryCaches()\n",
      "CALL apoc.warmup.run()\n",
      "MATCH (p1:Pangenome)<-[:IS_IN_PANGENOME]-(f1:Family)-[:HAS_PARTITION]->(s1:Partition) WITH p1, f1, s1 MATCH (f1)-[IS_SIMILAR]-(f2:Family) -[:HAS_PARTITION]->(s2) WITH p1, f1, s1, s2, f2 MATCH (p2:Pangenome)<-[IS_IN_PANGENOME]-(f2) WHERE p1.name <> p2.name RETURN p1.name, f1.name, s1.partition, p2.name, f2.name, s2.partition\n",
      "CALL db.clearQueryCaches()\n",
      "CALL apoc.warmup.run()\n",
      "MATCH (p1:Pangenome)<-[:IS_IN_PANGENOME]-(f1:Family)-[:HAS_PARTITION]->(s1:Partition) WITH p1, f1, s1 MATCH (f1)-[IS_SIMILAR]-(f2:Family) -[:HAS_PARTITION]->(s2) WITH p1, f1, s1, s2, f2 MATCH (p2:Pangenome)<-[IS_IN_PANGENOME]-(f2) WHERE p1.name <> p2.name RETURN p1.name, f1.name, s1.partition, p2.name, f2.name, s2.partition\n",
      "CALL db.clearQueryCaches()\n",
      "CALL apoc.warmup.run()\n",
      "MATCH (p1:Pangenome)<-[:IS_IN_PANGENOME]-(f1:Family)-[:HAS_PARTITION]->(s1:Partition) WITH p1, f1, s1 MATCH (f1)-[IS_SIMILAR]-(f2:Family) -[:HAS_PARTITION]->(s2) WITH p1, f1, s1, s2, f2 MATCH (p2:Pangenome)<-[IS_IN_PANGENOME]-(f2) WHERE p1.name <> p2.name RETURN p1.name, f1.name, s1.partition, p2.name, f2.name, s2.partition\n",
      "CALL db.clearQueryCaches()\n",
      "CALL apoc.warmup.run()\n",
      "MATCH (p1:Pangenome)<-[:IS_IN_PANGENOME]-(f1:Family)-[:HAS_PARTITION]->(s1:Partition) WITH p1, f1, s1 MATCH (f1)-[IS_SIMILAR]-(f2:Family) -[:HAS_PARTITION]->(s2) WITH p1, f1, s1, s2, f2 MATCH (p2:Pangenome)<-[IS_IN_PANGENOME]-(f2) WHERE p1.name <> p2.name RETURN p1.name, f1.name, s1.partition, p2.name, f2.name, s2.partition\n",
      "CALL db.clearQueryCaches()\n",
      "CALL apoc.warmup.run()\n",
      "MATCH a=(p:Partition)<-[HAS_PARTITION]-(f1:Family) -[:IS_IN_FAMILY]-(g:Gene)-[:IS_IN_RGP]-(r:RGP)-[:IS_IN_SPOT]-(s:Spot) WITH a,p,f1,g,r,s MATCH b=(z1:Pangenome)-[:IS_IN_PANGENOME]-(f1)-[:IS_SIMILAR]-(f2:Family)-[:IS_IN_PANGENOME]-(z2:Pangenome) WHERE f1.annotation IS NOT NULL AND z1<>z2 WITH a,b,p,f1,g,r,s,z1,z2,f2 MATCH c=(f2)-[:HAS_PARTITION]-(p2)RETURN a,b,c\n",
      "CALL db.clearQueryCaches()\n",
      "CALL apoc.warmup.run()\n",
      "MATCH a=(p:Partition)<-[HAS_PARTITION]-(f1:Family) -[:IS_IN_FAMILY]-(g:Gene)-[:IS_IN_RGP]-(r:RGP)-[:IS_IN_SPOT]-(s:Spot) WITH a,p,f1,g,r,s MATCH b=(z1:Pangenome)-[:IS_IN_PANGENOME]-(f1)-[:IS_SIMILAR]-(f2:Family)-[:IS_IN_PANGENOME]-(z2:Pangenome) WHERE f1.annotation IS NOT NULL AND z1<>z2 WITH a,b,p,f1,g,r,s,z1,z2,f2 MATCH c=(f2)-[:HAS_PARTITION]-(p2)RETURN a,b,c\n",
      "CALL db.clearQueryCaches()\n",
      "CALL apoc.warmup.run()\n",
      "MATCH a=(p:Partition)<-[HAS_PARTITION]-(f1:Family) -[:IS_IN_FAMILY]-(g:Gene)-[:IS_IN_RGP]-(r:RGP)-[:IS_IN_SPOT]-(s:Spot) WITH a,p,f1,g,r,s MATCH b=(z1:Pangenome)-[:IS_IN_PANGENOME]-(f1)-[:IS_SIMILAR]-(f2:Family)-[:IS_IN_PANGENOME]-(z2:Pangenome) WHERE f1.annotation IS NOT NULL AND z1<>z2 WITH a,b,p,f1,g,r,s,z1,z2,f2 MATCH c=(f2)-[:HAS_PARTITION]-(p2)RETURN a,b,c\n",
      "CALL db.clearQueryCaches()\n",
      "CALL apoc.warmup.run()\n",
      "MATCH a=(p:Partition)<-[HAS_PARTITION]-(f1:Family) -[:IS_IN_FAMILY]-(g:Gene)-[:IS_IN_RGP]-(r:RGP)-[:IS_IN_SPOT]-(s:Spot) WITH a,p,f1,g,r,s MATCH b=(z1:Pangenome)-[:IS_IN_PANGENOME]-(f1)-[:IS_SIMILAR]-(f2:Family)-[:IS_IN_PANGENOME]-(z2:Pangenome) WHERE f1.annotation IS NOT NULL AND z1<>z2 WITH a,b,p,f1,g,r,s,z1,z2,f2 MATCH c=(f2)-[:HAS_PARTITION]-(p2)RETURN a,b,c\n",
      "CALL db.clearQueryCaches()\n",
      "CALL apoc.warmup.run()\n",
      "MATCH a=(p:Partition)<-[HAS_PARTITION]-(f1:Family) -[:IS_IN_FAMILY]-(g:Gene)-[:IS_IN_RGP]-(r:RGP)-[:IS_IN_SPOT]-(s:Spot) WITH a,p,f1,g,r,s MATCH b=(z1:Pangenome)-[:IS_IN_PANGENOME]-(f1)-[:IS_SIMILAR]-(f2:Family)-[:IS_IN_PANGENOME]-(z2:Pangenome) WHERE f1.annotation IS NOT NULL AND z1<>z2 WITH a,b,p,f1,g,r,s,z1,z2,f2 MATCH c=(f2)-[:HAS_PARTITION]-(p2)RETURN a,b,c\n",
      "CALL db.clearQueryCaches()\n",
      "CALL apoc.warmup.run()\n",
      "MATCH a=(p:Partition)<-[HAS_PARTITION]-(f1:Family) -[:IS_IN_FAMILY]-(g:Gene)-[:IS_IN_RGP]-(r:RGP)-[:IS_IN_SPOT]-(s:Spot) WITH a,p,f1,g,r,s MATCH b=(z1:Pangenome)-[:IS_IN_PANGENOME]-(f1)-[:IS_SIMILAR]-(f2:Family)-[:IS_IN_PANGENOME]-(z2:Pangenome) WHERE f1.annotation IS NOT NULL AND z1<>z2 WITH a,b,p,f1,g,r,s,z1,z2,f2 MATCH c=(f2)-[:HAS_PARTITION]-(p2)RETURN a,b,c\n",
      "CALL db.clearQueryCaches()\n",
      "CALL apoc.warmup.run()\n",
      "MATCH a=(p:Partition)<-[HAS_PARTITION]-(f1:Family) -[:IS_IN_FAMILY]-(g:Gene)-[:IS_IN_RGP]-(r:RGP)-[:IS_IN_SPOT]-(s:Spot) WITH a,p,f1,g,r,s MATCH b=(z1:Pangenome)-[:IS_IN_PANGENOME]-(f1)-[:IS_SIMILAR]-(f2:Family)-[:IS_IN_PANGENOME]-(z2:Pangenome) WHERE f1.annotation IS NOT NULL AND z1<>z2 WITH a,b,p,f1,g,r,s,z1,z2,f2 MATCH c=(f2)-[:HAS_PARTITION]-(p2)RETURN a,b,c\n",
      "CALL db.clearQueryCaches()\n",
      "CALL apoc.warmup.run()\n",
      "MATCH a=(p:Partition)<-[HAS_PARTITION]-(f1:Family) -[:IS_IN_FAMILY]-(g:Gene)-[:IS_IN_RGP]-(r:RGP)-[:IS_IN_SPOT]-(s:Spot) WITH a,p,f1,g,r,s MATCH b=(z1:Pangenome)-[:IS_IN_PANGENOME]-(f1)-[:IS_SIMILAR]-(f2:Family)-[:IS_IN_PANGENOME]-(z2:Pangenome) WHERE f1.annotation IS NOT NULL AND z1<>z2 WITH a,b,p,f1,g,r,s,z1,z2,f2 MATCH c=(f2)-[:HAS_PARTITION]-(p2)RETURN a,b,c\n",
      "CALL db.clearQueryCaches()\n",
      "CALL apoc.warmup.run()\n",
      "MATCH a=(p:Partition)<-[HAS_PARTITION]-(f1:Family) -[:IS_IN_FAMILY]-(g:Gene)-[:IS_IN_RGP]-(r:RGP)-[:IS_IN_SPOT]-(s:Spot) WITH a,p,f1,g,r,s MATCH b=(z1:Pangenome)-[:IS_IN_PANGENOME]-(f1)-[:IS_SIMILAR]-(f2:Family)-[:IS_IN_PANGENOME]-(z2:Pangenome) WHERE f1.annotation IS NOT NULL AND z1<>z2 WITH a,b,p,f1,g,r,s,z1,z2,f2 MATCH c=(f2)-[:HAS_PARTITION]-(p2)RETURN a,b,c\n",
      "CALL db.clearQueryCaches()\n",
      "CALL apoc.warmup.run()\n",
      "MATCH a=(p:Partition)<-[HAS_PARTITION]-(f1:Family) -[:IS_IN_FAMILY]-(g:Gene)-[:IS_IN_RGP]-(r:RGP)-[:IS_IN_SPOT]-(s:Spot) WITH a,p,f1,g,r,s MATCH b=(z1:Pangenome)-[:IS_IN_PANGENOME]-(f1)-[:IS_SIMILAR]-(f2:Family)-[:IS_IN_PANGENOME]-(z2:Pangenome) WHERE f1.annotation IS NOT NULL AND z1<>z2 WITH a,b,p,f1,g,r,s,z1,z2,f2 MATCH c=(f2)-[:HAS_PARTITION]-(p2)RETURN a,b,c\n",
      "CALL db.clearQueryCaches()\n",
      "CALL apoc.warmup.run()\n",
      "MATCH (m1:Module)<-[:IS_IN_MODULE]-(f1:Family)-[:IS_IN_PANGENOME]->(p1:Pangenome) WITH f1, m1, p1 MATCH (p2:Pangenome)<-[:IS_IN_PANGENOME]-(f2:Family)-[:IS_SIMILAR]-(f1) WITH f1, m1, p1, f2, p2 MATCH (f2)-[:IS_IN_MODULE]-(m2:Module) WHERE f1.annotation IS NOT NULL AND p1 <> p2 RETURN p1, p2, m1, m2, f1, f2\n",
      "CALL db.clearQueryCaches()\n",
      "CALL apoc.warmup.run()\n",
      "MATCH (m1:Module)<-[:IS_IN_MODULE]-(f1:Family)-[:IS_IN_PANGENOME]->(p1:Pangenome) WITH f1, m1, p1 MATCH (p2:Pangenome)<-[:IS_IN_PANGENOME]-(f2:Family)-[:IS_SIMILAR]-(f1) WITH f1, m1, p1, f2, p2 MATCH (f2)-[:IS_IN_MODULE]-(m2:Module) WHERE f1.annotation IS NOT NULL AND p1 <> p2 RETURN p1, p2, m1, m2, f1, f2\n",
      "CALL db.clearQueryCaches()\n",
      "CALL apoc.warmup.run()\n",
      "MATCH (m1:Module)<-[:IS_IN_MODULE]-(f1:Family)-[:IS_IN_PANGENOME]->(p1:Pangenome) WITH f1, m1, p1 MATCH (p2:Pangenome)<-[:IS_IN_PANGENOME]-(f2:Family)-[:IS_SIMILAR]-(f1) WITH f1, m1, p1, f2, p2 MATCH (f2)-[:IS_IN_MODULE]-(m2:Module) WHERE f1.annotation IS NOT NULL AND p1 <> p2 RETURN p1, p2, m1, m2, f1, f2\n",
      "CALL db.clearQueryCaches()\n",
      "CALL apoc.warmup.run()\n",
      "MATCH (m1:Module)<-[:IS_IN_MODULE]-(f1:Family)-[:IS_IN_PANGENOME]->(p1:Pangenome) WITH f1, m1, p1 MATCH (p2:Pangenome)<-[:IS_IN_PANGENOME]-(f2:Family)-[:IS_SIMILAR]-(f1) WITH f1, m1, p1, f2, p2 MATCH (f2)-[:IS_IN_MODULE]-(m2:Module) WHERE f1.annotation IS NOT NULL AND p1 <> p2 RETURN p1, p2, m1, m2, f1, f2\n",
      "CALL db.clearQueryCaches()\n",
      "CALL apoc.warmup.run()\n",
      "MATCH (m1:Module)<-[:IS_IN_MODULE]-(f1:Family)-[:IS_IN_PANGENOME]->(p1:Pangenome) WITH f1, m1, p1 MATCH (p2:Pangenome)<-[:IS_IN_PANGENOME]-(f2:Family)-[:IS_SIMILAR]-(f1) WITH f1, m1, p1, f2, p2 MATCH (f2)-[:IS_IN_MODULE]-(m2:Module) WHERE f1.annotation IS NOT NULL AND p1 <> p2 RETURN p1, p2, m1, m2, f1, f2\n",
      "CALL db.clearQueryCaches()\n",
      "CALL apoc.warmup.run()\n",
      "MATCH (m1:Module)<-[:IS_IN_MODULE]-(f1:Family)-[:IS_IN_PANGENOME]->(p1:Pangenome) WITH f1, m1, p1 MATCH (p2:Pangenome)<-[:IS_IN_PANGENOME]-(f2:Family)-[:IS_SIMILAR]-(f1) WITH f1, m1, p1, f2, p2 MATCH (f2)-[:IS_IN_MODULE]-(m2:Module) WHERE f1.annotation IS NOT NULL AND p1 <> p2 RETURN p1, p2, m1, m2, f1, f2\n",
      "CALL db.clearQueryCaches()\n",
      "CALL apoc.warmup.run()\n",
      "MATCH (m1:Module)<-[:IS_IN_MODULE]-(f1:Family)-[:IS_IN_PANGENOME]->(p1:Pangenome) WITH f1, m1, p1 MATCH (p2:Pangenome)<-[:IS_IN_PANGENOME]-(f2:Family)-[:IS_SIMILAR]-(f1) WITH f1, m1, p1, f2, p2 MATCH (f2)-[:IS_IN_MODULE]-(m2:Module) WHERE f1.annotation IS NOT NULL AND p1 <> p2 RETURN p1, p2, m1, m2, f1, f2\n",
      "CALL db.clearQueryCaches()\n",
      "CALL apoc.warmup.run()\n",
      "MATCH (m1:Module)<-[:IS_IN_MODULE]-(f1:Family)-[:IS_IN_PANGENOME]->(p1:Pangenome) WITH f1, m1, p1 MATCH (p2:Pangenome)<-[:IS_IN_PANGENOME]-(f2:Family)-[:IS_SIMILAR]-(f1) WITH f1, m1, p1, f2, p2 MATCH (f2)-[:IS_IN_MODULE]-(m2:Module) WHERE f1.annotation IS NOT NULL AND p1 <> p2 RETURN p1, p2, m1, m2, f1, f2\n",
      "CALL db.clearQueryCaches()\n",
      "CALL apoc.warmup.run()\n",
      "MATCH (m1:Module)<-[:IS_IN_MODULE]-(f1:Family)-[:IS_IN_PANGENOME]->(p1:Pangenome) WITH f1, m1, p1 MATCH (p2:Pangenome)<-[:IS_IN_PANGENOME]-(f2:Family)-[:IS_SIMILAR]-(f1) WITH f1, m1, p1, f2, p2 MATCH (f2)-[:IS_IN_MODULE]-(m2:Module) WHERE f1.annotation IS NOT NULL AND p1 <> p2 RETURN p1, p2, m1, m2, f1, f2\n",
      "CALL db.clearQueryCaches()\n",
      "CALL apoc.warmup.run()\n",
      "MATCH (m1:Module)<-[:IS_IN_MODULE]-(f1:Family)-[:IS_IN_PANGENOME]->(p1:Pangenome) WITH f1, m1, p1 MATCH (p2:Pangenome)<-[:IS_IN_PANGENOME]-(f2:Family)-[:IS_SIMILAR]-(f1) WITH f1, m1, p1, f2, p2 MATCH (f2)-[:IS_IN_MODULE]-(m2:Module) WHERE f1.annotation IS NOT NULL AND p1 <> p2 RETURN p1, p2, m1, m2, f1, f2\n",
      "CALL db.clearQueryCaches()\n",
      "CALL apoc.warmup.run()\n",
      "MATCH (f1:Family)-[:HAS_PARTITION]->(s1:Partition) WITH f1, s1 MATCH (f1)-[r1:IS_SIMILAR]->(f2) -[:HAS_PARTITION]->(s2:Partition) WHERE r1.identity >= 0.8 AND r1.coverage >= 0.8 RETURN f1.name, s1.partition, f2.name, s2.partition, r1.identity, r1.coverage,f1.annotation, f2.annotation ORDER BY r1.identity LIMIT 10\n",
      "CALL db.clearQueryCaches()\n",
      "CALL apoc.warmup.run()\n",
      "MATCH (f1:Family)-[:HAS_PARTITION]->(s1:Partition) WITH f1, s1 MATCH (f1)-[r1:IS_SIMILAR]->(f2) -[:HAS_PARTITION]->(s2:Partition) WHERE r1.identity >= 0.8 AND r1.coverage >= 0.8 RETURN f1.name, s1.partition, f2.name, s2.partition, r1.identity, r1.coverage,f1.annotation, f2.annotation ORDER BY r1.identity LIMIT 10\n",
      "CALL db.clearQueryCaches()\n",
      "CALL apoc.warmup.run()\n",
      "MATCH (f1:Family)-[:HAS_PARTITION]->(s1:Partition) WITH f1, s1 MATCH (f1)-[r1:IS_SIMILAR]->(f2) -[:HAS_PARTITION]->(s2:Partition) WHERE r1.identity >= 0.8 AND r1.coverage >= 0.8 RETURN f1.name, s1.partition, f2.name, s2.partition, r1.identity, r1.coverage,f1.annotation, f2.annotation ORDER BY r1.identity LIMIT 10\n",
      "CALL db.clearQueryCaches()\n",
      "CALL apoc.warmup.run()\n",
      "MATCH (f1:Family)-[:HAS_PARTITION]->(s1:Partition) WITH f1, s1 MATCH (f1)-[r1:IS_SIMILAR]->(f2) -[:HAS_PARTITION]->(s2:Partition) WHERE r1.identity >= 0.8 AND r1.coverage >= 0.8 RETURN f1.name, s1.partition, f2.name, s2.partition, r1.identity, r1.coverage,f1.annotation, f2.annotation ORDER BY r1.identity LIMIT 10\n",
      "CALL db.clearQueryCaches()\n",
      "CALL apoc.warmup.run()\n",
      "MATCH (f1:Family)-[:HAS_PARTITION]->(s1:Partition) WITH f1, s1 MATCH (f1)-[r1:IS_SIMILAR]->(f2) -[:HAS_PARTITION]->(s2:Partition) WHERE r1.identity >= 0.8 AND r1.coverage >= 0.8 RETURN f1.name, s1.partition, f2.name, s2.partition, r1.identity, r1.coverage,f1.annotation, f2.annotation ORDER BY r1.identity LIMIT 10\n",
      "CALL db.clearQueryCaches()\n",
      "CALL apoc.warmup.run()\n",
      "MATCH (f1:Family)-[:HAS_PARTITION]->(s1:Partition) WITH f1, s1 MATCH (f1)-[r1:IS_SIMILAR]->(f2) -[:HAS_PARTITION]->(s2:Partition) WHERE r1.identity >= 0.8 AND r1.coverage >= 0.8 RETURN f1.name, s1.partition, f2.name, s2.partition, r1.identity, r1.coverage,f1.annotation, f2.annotation ORDER BY r1.identity LIMIT 10\n",
      "CALL db.clearQueryCaches()\n",
      "CALL apoc.warmup.run()\n",
      "MATCH (f1:Family)-[:HAS_PARTITION]->(s1:Partition) WITH f1, s1 MATCH (f1)-[r1:IS_SIMILAR]->(f2) -[:HAS_PARTITION]->(s2:Partition) WHERE r1.identity >= 0.8 AND r1.coverage >= 0.8 RETURN f1.name, s1.partition, f2.name, s2.partition, r1.identity, r1.coverage,f1.annotation, f2.annotation ORDER BY r1.identity LIMIT 10\n",
      "CALL db.clearQueryCaches()\n",
      "CALL apoc.warmup.run()\n",
      "MATCH (f1:Family)-[:HAS_PARTITION]->(s1:Partition) WITH f1, s1 MATCH (f1)-[r1:IS_SIMILAR]->(f2) -[:HAS_PARTITION]->(s2:Partition) WHERE r1.identity >= 0.8 AND r1.coverage >= 0.8 RETURN f1.name, s1.partition, f2.name, s2.partition, r1.identity, r1.coverage,f1.annotation, f2.annotation ORDER BY r1.identity LIMIT 10\n",
      "CALL db.clearQueryCaches()\n",
      "CALL apoc.warmup.run()\n",
      "MATCH (f1:Family)-[:HAS_PARTITION]->(s1:Partition) WITH f1, s1 MATCH (f1)-[r1:IS_SIMILAR]->(f2) -[:HAS_PARTITION]->(s2:Partition) WHERE r1.identity >= 0.8 AND r1.coverage >= 0.8 RETURN f1.name, s1.partition, f2.name, s2.partition, r1.identity, r1.coverage,f1.annotation, f2.annotation ORDER BY r1.identity LIMIT 10\n",
      "CALL db.clearQueryCaches()\n",
      "CALL apoc.warmup.run()\n",
      "MATCH (f1:Family)-[:HAS_PARTITION]->(s1:Partition) WITH f1, s1 MATCH (f1)-[r1:IS_SIMILAR]->(f2) -[:HAS_PARTITION]->(s2:Partition) WHERE r1.identity >= 0.8 AND r1.coverage >= 0.8 RETURN f1.name, s1.partition, f2.name, s2.partition, r1.identity, r1.coverage,f1.annotation, f2.annotation ORDER BY r1.identity LIMIT 10\n",
      "CALL db.clearQueryCaches()\n"
     ]
    },
    {
     "data": {
      "text/plain": "         Mean     Stdev   Mediane       Min       Max\nQ1   0.043050  0.003106  0.042875  0.037794  0.046781\nQ2   0.039839  0.002843  0.039303  0.036775  0.045423\nQ3   0.556492  0.041772  0.532297  0.521851  0.635195\nQ4   1.186968  0.115383  1.137031  1.100517  1.444945\nQ5   0.042844  0.005650  0.041675  0.037320  0.056931\nQ6   0.039438  0.007757  0.037768  0.032563  0.060707\nQ7   2.385238  0.348206  2.291893  2.050604  3.088853\nQ8   1.111279  0.025456  1.106841  1.073579  1.160115\nQ9   0.053641  0.005251  0.051907  0.048528  0.064753\nQ10  0.083611  0.005362  0.082206  0.078561  0.094062",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Mean</th>\n      <th>Stdev</th>\n      <th>Mediane</th>\n      <th>Min</th>\n      <th>Max</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Q1</th>\n      <td>0.043050</td>\n      <td>0.003106</td>\n      <td>0.042875</td>\n      <td>0.037794</td>\n      <td>0.046781</td>\n    </tr>\n    <tr>\n      <th>Q2</th>\n      <td>0.039839</td>\n      <td>0.002843</td>\n      <td>0.039303</td>\n      <td>0.036775</td>\n      <td>0.045423</td>\n    </tr>\n    <tr>\n      <th>Q3</th>\n      <td>0.556492</td>\n      <td>0.041772</td>\n      <td>0.532297</td>\n      <td>0.521851</td>\n      <td>0.635195</td>\n    </tr>\n    <tr>\n      <th>Q4</th>\n      <td>1.186968</td>\n      <td>0.115383</td>\n      <td>1.137031</td>\n      <td>1.100517</td>\n      <td>1.444945</td>\n    </tr>\n    <tr>\n      <th>Q5</th>\n      <td>0.042844</td>\n      <td>0.005650</td>\n      <td>0.041675</td>\n      <td>0.037320</td>\n      <td>0.056931</td>\n    </tr>\n    <tr>\n      <th>Q6</th>\n      <td>0.039438</td>\n      <td>0.007757</td>\n      <td>0.037768</td>\n      <td>0.032563</td>\n      <td>0.060707</td>\n    </tr>\n    <tr>\n      <th>Q7</th>\n      <td>2.385238</td>\n      <td>0.348206</td>\n      <td>2.291893</td>\n      <td>2.050604</td>\n      <td>3.088853</td>\n    </tr>\n    <tr>\n      <th>Q8</th>\n      <td>1.111279</td>\n      <td>0.025456</td>\n      <td>1.106841</td>\n      <td>1.073579</td>\n      <td>1.160115</td>\n    </tr>\n    <tr>\n      <th>Q9</th>\n      <td>0.053641</td>\n      <td>0.005251</td>\n      <td>0.051907</td>\n      <td>0.048528</td>\n      <td>0.064753</td>\n    </tr>\n    <tr>\n      <th>Q10</th>\n      <td>0.083611</td>\n      <td>0.005362</td>\n      <td>0.082206</td>\n      <td>0.078561</td>\n      <td>0.094062</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from time import time\n",
    "from datetime import timedelta\n",
    "\n",
    "def launch_query(query):\n",
    "    print(query)\n",
    "    try:\n",
    "        res = graph.run(query)\n",
    "    except Exception as errror:\n",
    "        raise Exception(f\"Query : '{query}' failed because of the following errror\\n{errror}\")\n",
    "    else:\n",
    "        return  res\n",
    "\n",
    "def launch_WF():\n",
    "    from script.python.wf import WF\n",
    "    from statistics import mean, median, stdev\n",
    "    import pandas as pd\n",
    "\n",
    "    nb_rep = 10\n",
    "    stat_dict = {}\n",
    "    for q_id, query in WF.items():\n",
    "        list_q_time = []\n",
    "        for i in range(nb_rep):\n",
    "            launch_query(\"CALL apoc.warmup.run()\")\n",
    "            begin_time = time()\n",
    "            res = launch_query(query)\n",
    "            q_time = time() - begin_time\n",
    "            list_q_time.append(q_time)\n",
    "            launch_query(\"CALL db.clearQueryCaches()\")\n",
    "        stat_dict[q_id] = [mean(list_q_time), stdev(list_q_time), median(list_q_time),\n",
    "                           min(list_q_time), max(list_q_time)]\n",
    "    return pd.DataFrame.from_dict(stat_dict, orient='index', columns=[\"Mean\", \"Stdev\", \"Mediane\", \"Min\", \"Max\"])\n",
    "\n",
    "launch_WF()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "id": "bedc22b2",
   "metadata": {},
   "source": [
    "# Suplementary code\n",
    "## Genomes downloading\n",
    "Step do download genomes necessary to consctruct pangenomes. Note that genomes database are in constant evolution and your pangenome could be different than our."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!mkdir $GBFF\n",
    "!genome_updater.sh -d genbank,refseq -M gtdb -T \"s__Acinetobacter baumannii\" -f assembly_report.txt,genomic.gbff.gz -l \"complete genome\" -o $GBFF/Acinetobacter.baumannii -t $CPU\n",
    "!genome_updater.sh -d genbank,refseq -M gtdb -T \"s__Enterobacter bugandensis\" -f assembly_report.txt,genomic.gbff.gz -o $GBFF/Enterobacter.bugandensis -t $CPU\n",
    "!genome_updater.sh -d genbank,refseq -M gtdb -T \"s__Enterobacter cloacae\" -f assembly_report.txt,genomic.gbff.gz -o $GBFF/Enterobacter.cloacae -t $CPU\n",
    "!genome_updater.sh -d genbank,refseq -M gtdb -T \"s__Enterobacter hormaechei_A\" -f assembly_report.txt,genomic.gbff.gz -l \"complete genome\" -o $GBFF/Enterobacter.hormaechei_A -t $CPU\n",
    "!genome_updater.sh -d genbank,refseq -M gtdb -T \"s__Enterobacter kobei\" -f assembly_report.txt,genomic.gbff.gz -o $GBFF/Enterobacter.kobei -t $CPU\n",
    "!genome_updater.sh -d genbank,refseq -M gtdb -T \"s__Enterobacter roggenkampii\" -f assembly_report.txt,genomic.gbff.gz -o $GBFF/Enterobacter.roggenkampii -t $CPU\n",
    "!genome_updater.sh -d genbank,refseq -M gtdb -T \"s__Enterococcus_B faecium\" -f assembly_report.txt,genomic.gbff.gz -l \"complete genome\" -o $GBFF/Enterococcus_B.faecium -t $CPU\n",
    "!genome_updater.sh -d genbank,refseq -M gtdb -T \"s__Klebsiella pneumoniae\" -f assembly_report.txt,genomic.gbff.gz -l \"complete genome\" -A 600 -o $GBFF/Klebsiella.pneumoniae -t $CPU\n",
    "!genome_updater.sh -d genbank,refseq -M gtdb -T \"s__Pseudomonas aeruginosa\" -f assembly_report.txt,genomic.gbff.gz -l \"complete genome\" -o $GBFF/Pseudomonas.aeruginosa -t $CPU\n",
    "!genome_updater.sh -d genbank,refseq -M gtdb -T \"s__Staphylococcus aureus\" -f assembly_report.txt,genomic.gbff.gz -l \"complete genome\" -A 600 -o $GBFF/Staphylococcus.aureus -t $CPU"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Pangenome construction with PPanGGOLiN\n",
    "### Generate list of genomes"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "! for sp in $(ls $GBFF); do echo for path in $(ls $GBFF/$sp/*/files/*.gbff.gz);do genome=$(echo $path | cut -d'/' -f6 | cut -d. -f1,2); echo $genome $(pwd)/$path | sed 's/\\s/\\t/'; done > $PANGENOMES/$sp.list; done"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Generate pangenomes"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!ppanggolin all --anno data/pangenomes/Acinetobacter.baumannii.list -o data/pangenomes/Acinetobacter.baumannii --only_pangenome -c 4\n",
    "!ppanggolin all --anno data/pangenomes/Enterobacter.bugandensis.list -o data/pangenomes/Enterobacter.bugandensis --only_pangenome -c 4\n",
    "!ppanggolin all --anno data/pangenomes/Enterobacter.cloacae.list -o data/pangenomes/Enterobacter.cloacae --only_pangenome -c 4\n",
    "!ppanggolin all --anno data/pangenomes/Enterobacter.hormaechei_A.list -o data/pangenomes/Enterobacter.hormaechei_A --only_pangenome -c 4\n",
    "!ppanggolin all --anno data/pangenomes/Enterobacter.kobei.list -o data/pangenomes/Enterobacter.kobei --only_pangenome -c 4\n",
    "!ppanggolin all --anno data/pangenomes/Enterobacter.roggenkampii.list -o data/pangenomes/Enterobacter.roggenkampii --only_pangenome -c 4\n",
    "!ppanggolin all --anno data/pangenomes/Enterococcus_B.faecium.list -o data/pangenomes/Enterococcus_B.faecium --only_pangenome -c 4\n",
    "!ppanggolin all --anno data/pangenomes/Klebsiella.pneumoniae.list -o data/pangenomes/Klebsiella.pneumoniae --only_pangenome -c 4\n",
    "!ppanggolin all --anno data/pangenomes/Pseudomonas.aeruginosa.list -o data/pangenomes/Pseudomonas.aeruginosa --only_pangenome -c 4\n",
    "!ppanggolin all --anno data/pangenomes/Staphylococcus.aureus.list -o data/pangenomes/Staphylococcus.aureus --only_pangenome -c 4"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Pangenome AMR annotation with CARD database and RGI"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Clean to relaunch"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!rm -rf PPanGGOLiN\n",
    "!rm -rf $GBFF\n",
    "!conda env remove -n pangraph\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "id": "0bb48de1",
   "metadata": {},
   "source": [
    "# References\n",
    "We report here relevant references:\n",
    "1. author1, article1, journal1, year1, url1\n",
    "2. author2, article2, journal2, year2, url2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
